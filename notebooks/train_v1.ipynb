{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def seed_all(seed_value):\n",
    "    random.seed(seed_value) # Python\n",
    "    torch.manual_seed(seed_value) # cpu vars\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    if torch.cuda.is_available(): \n",
    "        torch.cuda.manual_seed(seed_value) # gpu variables\n",
    "\n",
    "seed = 56\n",
    "seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import copy\n",
    "from zipfile import ZipFile\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import optim\n",
    "from functools import partial\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.impute import KNNImputer, SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load train & test csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "final_df = pd.DataFrame()\n",
    "for f in os.listdir('../data/sharechat_recsys2023_data/train'):\n",
    "    final_df = pd.concat([final_df, pd.read_csv('../data/sharechat_recsys2023_data/train/{}'.format(f), \n",
    "                                                delimiter='\\t')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test = pd.read_csv('../data/sharechat_recsys2023_data/test/000000000000.csv'.format(f),\n",
    "                         delimiter='\\t')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "1. Categorical Data Preprocessing\n",
    "    * Assign 0 index to NAN values.\n",
    "    * Assign 1 index to Unknown category values observed during test time.\n",
    "    * Assign further indices to unique categories in train data.\n",
    "    \n",
    "    \n",
    "2. Binary Features Preprocessing\n",
    "    * same as categorical features\n",
    "    \n",
    "    \n",
    "3. Numerical Features Processing\n",
    "    * We did calculated corr between is_installed and numerical columns and found certain numerical columns have very low corr score so we removed those cols from train and test set.\n",
    "    ```\n",
    "    ['f_7','f_5', 'f_9', 'f_13', 'f_15', 'f_18', 'f_26', \n",
    "                  'f_27', 'f_28', 'f_29', 'f_31', 'f_30', 'f_60', 'f_67',\n",
    "                  'f_44', 'f_46', 'f_75', 'f_76',\n",
    "                 ]\n",
    "    ```\n",
    "    * We imputed NaN values in numerical columns by the mean of the column\n",
    "    * We performed numerical columns normalization using MinMaxScaler\n",
    "    * We considered numerical column which had very less no unique values we considered them as categorical features as well during training and testing  \n",
    "    ```\n",
    "    ['f_79', 'f_78', 'f_77', 'f_76', 'f_75', 'f_74', 'f_73', 'f_72']\n",
    "    ```\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAN_CATEG = -1\n",
    "NAN_IDX = 0\n",
    "UNK_IDX = 1\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_remove = ['f_7','f_5', 'f_9', 'f_13', 'f_15', 'f_18', 'f_26', \n",
    "                  'f_27', 'f_28', 'f_29', 'f_31', 'f_30', 'f_60', 'f_67',\n",
    "                  'f_44', 'f_46', 'f_75', 'f_76',\n",
    "                 ]\n",
    "\n",
    "num_to_categ_col = ['f_79', 'f_78', 'f_77', 'f_76', 'f_75', 'f_74', 'f_73', 'f_72']\n",
    "\n",
    "original_id_cols = ['f_0']\n",
    "original_date_cols = ['f_1']\n",
    "original_categ_cols = [f'f_{id}' for id in range(2, 32+1)]\n",
    "original_binary_cols = [f'f_{id}' for id in range(33, 41+1)]\n",
    "original_num_cols = [f'f_{id}' for id in range(42, 79+1)]\n",
    "original_label_cols = ['is_clicked', 'is_installed']\n",
    "\n",
    "\n",
    "revised_id_cols = ['f_0']\n",
    "revised_date_cols = ['f_1']\n",
    "revised_categ_cols = list((set([f'f_{id}' for id in range(2, 32+1)]) | set(num_to_categ_col)) - set(cols_to_remove))\n",
    "revised_categ_cols = sorted(revised_categ_cols)\n",
    "revised_binary_cols = list(set([f'f_{id}' for id in range(33, 41+1)]) -  set(cols_to_remove))\n",
    "revised_binary_cols = sorted(revised_binary_cols)\n",
    "revised_num_cols = list(set([f'f_{id}' for id in range(42, 79+1)]) - set(cols_to_remove) - set(num_to_categ_col))\n",
    "revised_label_cols = ['is_clicked', 'is_installed']\n",
    "\n",
    "categ_feat_to_id = {}\n",
    "for col in revised_categ_cols:\n",
    "    start_idx = 2 if final_df[col].isna().sum() == 0 else 1\n",
    "    categ_feat_to_id[col] = {feat:(id + start_idx if feat > -1 else 0)\n",
    "                             for id, feat in enumerate(sorted(final_df[col].fillna(NAN_CATEG).unique()))}\n",
    "    \n",
    "\n",
    "# when using binary feats with embeddings\n",
    "for col in revised_binary_cols:\n",
    "    start_idx = 2 if final_df[col].isna().sum() == 0 else 1\n",
    "    categ_feat_to_id[col] = {feat:(id + start_idx if feat > -1 else 0)\n",
    "                             for id, feat in enumerate(sorted(final_df[col].fillna(NAN_CATEG).unique()))}\n",
    "    \n",
    "\n",
    "    \n",
    "def get_feat_ids(df, cols):\n",
    "    \"\"\"\n",
    "    convert categorical feature to Ids \n",
    "    \"\"\"\n",
    "    _df = df.copy()\n",
    "    for col in cols:\n",
    "        _df[col] = _df[col].apply(lambda x: (categ_feat_to_id[col][x] \n",
    "                                             if not math.isnan(x) else categ_feat_to_id[col][NAN_CATEG]) \n",
    "                                        if x in categ_feat_to_id[col] or math.isnan(x) else UNK_IDX\n",
    "        )\n",
    "    return _df\n",
    "\n",
    "def rem_nan_values(train_df, test_df, cols):\n",
    "    \"\"\"\n",
    "    Replace NaN with 0\n",
    "    \"\"\"\n",
    "    train_df_col_mean = train_df[cols].mean()\n",
    "    train_df[cols] = train_df[cols].fillna(train_df_col_mean)\n",
    "    test_df[cols] = test_df[cols].fillna(train_df_col_mean)\n",
    "    return train_df, test_df\n",
    "\n",
    "def binary_cols_processing(_df, cols):\n",
    "    \"\"\"\n",
    "    Replace 0 values with -1\n",
    "    \"\"\"\n",
    "    _df[cols] = _df[cols].replace(0, -1)\n",
    "    return _df\n",
    "\n",
    "def num_cols_processing(train_df, test_df, cols):\n",
    "    \"\"\"\n",
    "    Normalize/Standardize numerical columns\n",
    "    \"\"\"\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(train_df[cols])\n",
    "    train_df[cols] = scaler.transform(train_df[cols])\n",
    "    test_df[cols] = scaler.transform(test_df[cols])\n",
    "        \n",
    "    return train_df, test_df\n",
    "\n",
    "def imput_nan_values(train_df, test_df, cols):\n",
    "\n",
    "    unique_values = {col: len(train_df[col].unique().tolist()) for col in cols}\n",
    "    less_than_10 = [col for col, unique_count in unique_values.items() if unique_count < 20]\n",
    "    greater_than_10 = [col for col, unique_count in unique_values.items() if unique_count >= 20]\n",
    "\n",
    "    mean_imputer = SimpleImputer(strategy='mean')\n",
    "    mean_imputer.fit(train_df[greater_than_10])\n",
    "\n",
    "    train_df[greater_than_10] = mean_imputer.transform(train_df[greater_than_10])\n",
    "    test_df[greater_than_10] = mean_imputer.transform(test_df[greater_than_10])\n",
    "\n",
    "    knn_imputer = KNNImputer(n_neighbors=20, weights=\"uniform\")\n",
    "    knn_imputer.fit(train_df[less_than_10])\n",
    "    train_df[less_than_10] = knn_imputer.transform(train_df[less_than_10])\n",
    "    test_df[less_than_10] = knn_imputer.transform(test_df[less_than_10])\n",
    "    \n",
    "    return train_df, test_df\n",
    "    \n",
    "\n",
    "def cap_values(train_df, test_df, cols):\n",
    "    clips = pd.Series({col: train_df[col].quantile(0.85) for col in cols})\n",
    "    train_df[cols] = train_df.clip(upper=clips, axis=1)[cols]\n",
    "    test_df[cols] = test_df.clip(upper=clips, axis=1)[cols]\n",
    "    \n",
    "    return train_df, test_df\n",
    "\n",
    "def remove_unnecessary_columns(df, cols):\n",
    "    df = df.drop(cols, axis = 1)\n",
    "    return df\n",
    "\n",
    "def power_num_cols(df, cols):\n",
    "    _df = df.copy()\n",
    "    _df = pd.concat([_df, \n",
    "                     np.log1p(_df[cols]).add_suffix('_log'),\n",
    "                     np.sqrt(_df[cols]).add_suffix('_sqRoot')], axis=1)\n",
    "    cols = cols.extend(\n",
    "                            [col+'_log' for col in cols] +\n",
    "                            [col+'_sqRoot' for col in cols]\n",
    "                        )\n",
    "    return _df\n",
    "\n",
    "\n",
    "def preprocess_df(final_df, final_test):\n",
    "    \n",
    "    cols_to_remove = ['f_7','f_5', 'f_9', 'f_13', 'f_15', 'f_18', 'f_26', \n",
    "                      'f_27', 'f_28', 'f_29', 'f_31', 'f_30', 'f_60', 'f_67',\n",
    "                      'f_44', 'f_46', 'f_75', 'f_76'\n",
    "                     ]\n",
    "    \n",
    "    _final_df = remove_unnecessary_columns(final_df, cols_to_remove)\n",
    "    _test_df = remove_unnecessary_columns(final_test, cols_to_remove)\n",
    "\n",
    "    _final_df = get_feat_ids(_final_df, revised_categ_cols)\n",
    "    _test_df = get_feat_ids(_test_df, revised_categ_cols)\n",
    "        \n",
    "    _final_df, _test_df = rem_nan_values(_final_df, _test_df, revised_num_cols)\n",
    "    _final_df = binary_cols_processing(_final_df, revised_binary_cols)\n",
    "\n",
    "\n",
    "#     _test_df = rem_nan_values(_test_df, revised_num_cols)\n",
    "    _test_df = binary_cols_processing(_test_df, revised_binary_cols)\n",
    "\n",
    "    \n",
    "    _revised_num_cols = copy.deepcopy(revised_num_cols)\n",
    "    _final_df = power_num_cols(_final_df, _revised_num_cols)\n",
    "    print(revised_num_cols)\n",
    "    _test_df = power_num_cols(_test_df, revised_num_cols)\n",
    "    \n",
    "    _final_df, _test_df = num_cols_processing(_final_df, _test_df, revised_num_cols)\n",
    "    \n",
    "    return _final_df, _test_df\n",
    "\n",
    "\n",
    "def preprocess_df_bin(final_df, final_test):\n",
    "    \n",
    "    cols_to_remove = ['f_7','f_5', 'f_9', 'f_13', 'f_15', 'f_18', 'f_26',\n",
    "                      'f_27','f_28','f_29','f_31','f_30','f_60','f_67','f_44',\n",
    "                      'f_46','f_75','f_76'\n",
    "                     ]\n",
    "    \n",
    "    _final_df = remove_unnecessary_columns(final_df, cols_to_remove)\n",
    "    _test_df = remove_unnecessary_columns(final_test, cols_to_remove)\n",
    "\n",
    "    _final_df = get_feat_ids(_final_df, revised_categ_cols)\n",
    "    _final_df = get_feat_ids(_final_df, revised_binary_cols)\n",
    "    \n",
    "    _test_df = get_feat_ids(_test_df, revised_categ_cols)\n",
    "    _test_df = get_feat_ids(_test_df, revised_binary_cols)\n",
    "        \n",
    "    _final_df, _test_df = rem_nan_values(_final_df, _test_df, revised_num_cols)\n",
    "    \n",
    "    _final_df, _test_df = num_cols_processing(_final_df, _test_df, revised_num_cols)\n",
    "    \n",
    "    return _final_df, _test_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 22s, sys: 1min 58s, total: 3min 20s\n",
      "Wall time: 3min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "_final_df, _test_df = preprocess_df_bin(final_df, final_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rearranged_cols = ['f_0', 'f_1'] + \\\n",
    "                    list(revised_categ_cols) + \\\n",
    "                    list(revised_binary_cols) + \\\n",
    "                    list(revised_num_cols) + \\\n",
    "                    ['is_clicked', 'is_installed']\n",
    "_final_df = _final_df[rearranged_cols]\n",
    "_test_df = _test_df[rearranged_cols[:-2]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdsDataset(Dataset):\n",
    "    def __init__(self, df, test=False):\n",
    "        self.df = df\n",
    "        self.test = test\n",
    "        self.data = torch.from_numpy(df.values).to(torch.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.test:\n",
    "            return self.data[idx]\n",
    "        x = self.data[idx, :-2]\n",
    "        y = self.data[idx, -2:]\n",
    "        return x, y\n",
    "\n",
    "\n",
    "train_dataset = AdsDataset(_final_df)\n",
    "test_dataset = AdsDataset(_test_df, test=True)\n",
    "\n",
    "BS = 2048\n",
    "trainloader = DataLoader(train_dataset, batch_size=BS, shuffle=True)\n",
    "testloader = DataLoader(test_dataset, batch_size=len(_test_df))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "    \n",
    "    1. ModelV1\n",
    "    * Process categorical features through embedding -> concat the output\n",
    "    * Process binary features and numerical features through separate MLP layers\n",
    "    * Concatenate output of Embedding, binary and numerical MLP layers.\n",
    "    * Process concatenated output through separate MLP layers for `is_clicked` and `is_installed` logit output.\n",
    "    \n",
    "    2. ModelV1_BIN (Best Result Model) \n",
    "    Same as ModelV1, except binary features are also processed through Embeddings Instead of Dense layer.\n",
    "    \n",
    "    3. ModelV1_DEP\n",
    "    Same as ModelV1_BIN, except having seperate independent prediction of `is_installed` and `is_clicked`, in this model we prediction `is_installed` conditioned on `is_clicked`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelV1(nn.Module):\n",
    "    CATEG_COLS = revised_categ_cols\n",
    "    BINARY_COLS = revised_binary_cols\n",
    "    NUM_COLS = revised_num_cols\n",
    "    \n",
    "    START_COL_IDX = 2\n",
    "    \n",
    "    BINARY_COLS_IDX = range(START_COL_IDX + len(CATEG_COLS), START_COL_IDX + len(CATEG_COLS) + len(BINARY_COLS))\n",
    "    \n",
    "    NUM_COLS_IDX = range(BINARY_COLS_IDX[-1] + 1, BINARY_COLS_IDX[-1] + 1 + len(NUM_COLS))\n",
    "    \n",
    "    \n",
    "    CATEG_EMB_DIM = 32\n",
    "    BINARY_FC_OUT = 32\n",
    "    NUM_FC_OUT = 32\n",
    "\n",
    "    COMB_FC_OUT_1 = 256\n",
    "    NUM_CLASSES = 1\n",
    "\n",
    "    def __init__(self, df):\n",
    "        super(ModelV1, self).__init__()\n",
    "        # categ feats\n",
    "        cat_emb = [None]*len(self.CATEG_COLS)\n",
    "        self.emb_sizes = []\n",
    "        for col, col_idx in zip(self.CATEG_COLS, range(len(self.CATEG_COLS))):\n",
    "            n_emb = max(categ_feat_to_id[col].values())+1\n",
    "            emb_size = int(6 * (n_emb ** 0.25))\n",
    "            cat_emb[col_idx] = nn.Embedding(n_emb, emb_size).to(DEVICE)\n",
    "            self.emb_sizes.append(emb_size)\n",
    "            \n",
    "        self.cat_emb = nn.ModuleList(cat_emb)\n",
    "        \n",
    "        # binary_feats\n",
    "        self.bin_fc = nn.Sequential(\n",
    "            nn.Linear(len(self.BINARY_COLS), self.BINARY_FC_OUT),\n",
    "            nn.BatchNorm1d(self.BINARY_FC_OUT),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # numerical feats\n",
    "        self.num_fc = nn.Sequential(\n",
    "            nn.Linear(len(self.NUM_COLS), self.NUM_FC_OUT),\n",
    "            nn.BatchNorm1d(self.NUM_FC_OUT),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # final cls layers\n",
    "        self.fc_out_click = nn.Sequential(\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(\n",
    "                sum(self.emb_sizes) + self.BINARY_FC_OUT + self.NUM_FC_OUT,\n",
    "                self.COMB_FC_OUT_1\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.COMB_FC_OUT_1, self.NUM_CLASSES)\n",
    "        )\n",
    "\n",
    "        self.fc_out_install = nn.Sequential(\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(\n",
    "                sum(self.emb_sizes) + self.BINARY_FC_OUT + self.NUM_FC_OUT,\n",
    "                self.COMB_FC_OUT_1\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.COMB_FC_OUT_1, self.NUM_CLASSES)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, return_click = True):\n",
    "        cat_emb_out = [None]*len(self.CATEG_COLS)\n",
    "        for col_idx in range(len(self.CATEG_COLS)):\n",
    "            col_x = x[:, self.START_COL_IDX + col_idx].long()\n",
    "            cat_emb_out[col_idx] = self.cat_emb[col_idx](col_x)\n",
    "        cat_emb_out = torch.concat(cat_emb_out, axis=1)\n",
    "\n",
    "        bin_out = self.bin_fc(x[:, self.BINARY_COLS_IDX])\n",
    "        num_out = self.num_fc(x[:, self.NUM_COLS_IDX])\n",
    "\n",
    "        comb = torch.concat([cat_emb_out, bin_out, num_out], axis=1)\n",
    "        \n",
    "        install_logits = torch.sigmoid(self.fc_out_install(comb))\n",
    "        if(return_click):\n",
    "            click_logits = torch.sigmoid(self.fc_out_click(comb))\n",
    "        else:\n",
    "            click_logits = torch.zeros_like(install_logits)\n",
    "        return click_logits, install_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelV1_BIN(nn.Module):\n",
    "    CATEG_COLS = revised_categ_cols\n",
    "    BINARY_COLS = revised_binary_cols\n",
    "    NUM_COLS = revised_num_cols\n",
    "    \n",
    "    START_COL_IDX = 2\n",
    "    \n",
    "    BINARY_COLS_IDX = range(START_COL_IDX + len(CATEG_COLS), \n",
    "                            START_COL_IDX + len(CATEG_COLS) + len(BINARY_COLS))\n",
    "    \n",
    "    NUM_COLS_IDX = range(BINARY_COLS_IDX[-1] + 1, \n",
    "                         BINARY_COLS_IDX[-1] + 1 + len(NUM_COLS))\n",
    "    \n",
    "    CATEG_EMB_DIM = 32\n",
    "    BINARY_FC_OUT = 32\n",
    "    NUM_FC_OUT = 32\n",
    "\n",
    "    COMB_FC_OUT_1 = 256\n",
    "    NUM_CLASSES = 1\n",
    "\n",
    "    def __init__(self, df):\n",
    "        super(ModelV1_BIN, self).__init__()\n",
    "        # categ feats\n",
    "        cat_emb = [None]*len(self.CATEG_COLS)\n",
    "        self.emb_sizes = []\n",
    "        for col, col_idx in zip(self.CATEG_COLS, range(len(self.CATEG_COLS))):\n",
    "            n_emb = max(categ_feat_to_id[col].values())+1\n",
    "            emb_size = int(6 * (n_emb ** 0.25))\n",
    "            cat_emb[col_idx] = nn.Embedding(n_emb, emb_size).to(DEVICE)\n",
    "            self.emb_sizes.append(emb_size)\n",
    "            \n",
    "        self.cat_emb = nn.ModuleList(cat_emb)\n",
    "        \n",
    "\n",
    "        bin_emb = [None]*len(self.BINARY_COLS)\n",
    "        self.bin_emb_sizes = []\n",
    "        for col, col_idx in zip(self.BINARY_COLS, range(len(self.BINARY_COLS))):\n",
    "            n_emb = max(categ_feat_to_id[col].values())+1\n",
    "            self.bin_emb_sizes.append(1)\n",
    "            bin_emb[col_idx] = nn.Embedding(n_emb, 1).to(DEVICE)\n",
    "        self.bin_emb = nn.ModuleList(bin_emb)\n",
    "\n",
    "        # numerical feats\n",
    "        self.num_fc = nn.Sequential(\n",
    "            nn.Linear(len(self.NUM_COLS), self.NUM_FC_OUT),\n",
    "            nn.BatchNorm1d(self.NUM_FC_OUT),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # final cls layers\n",
    "        self.fc_out_click = nn.Sequential(\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(\n",
    "                sum(self.emb_sizes) + sum(self.bin_emb_sizes) + self.NUM_FC_OUT,\n",
    "                self.COMB_FC_OUT_1\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.COMB_FC_OUT_1, self.NUM_CLASSES)\n",
    "        )\n",
    "\n",
    "        self.fc_out_install = nn.Sequential(\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(\n",
    "                sum(self.emb_sizes) + sum(self.bin_emb_sizes) + self.NUM_FC_OUT,\n",
    "                self.COMB_FC_OUT_1\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.COMB_FC_OUT_1, self.NUM_CLASSES)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, return_click = True):\n",
    "        cat_emb_out = [None]*len(self.CATEG_COLS)\n",
    "        for col_idx in range(len(self.CATEG_COLS)):\n",
    "            col_x = x[:, self.START_COL_IDX + col_idx].long()\n",
    "            cat_emb_out[col_idx] = self.cat_emb[col_idx](col_x)\n",
    "        cat_emb_out = torch.concat(cat_emb_out, axis=1)\n",
    "\n",
    "        bin_emb_out = [None]*len(self.BINARY_COLS)\n",
    "        for col_idx in range(len(self.BINARY_COLS)):\n",
    "            col_x = x[:, self.START_COL_IDX + len(self.CATEG_COLS) + col_idx].long()\n",
    "            bin_emb_out[col_idx] = self.bin_emb[col_idx](col_x)\n",
    "        bin_emb_out = torch.concat(bin_emb_out, axis=1)\n",
    "        \n",
    "        num_out = self.num_fc(x[:, self.NUM_COLS_IDX])\n",
    "\n",
    "        comb = torch.concat([cat_emb_out, bin_emb_out, num_out], axis=1)\n",
    "        \n",
    "        install_logits = torch.sigmoid(self.fc_out_install(comb))\n",
    "        if(return_click):\n",
    "            click_logits = torch.sigmoid(self.fc_out_click(comb))\n",
    "        else:\n",
    "            click_logits = torch.zeros_like(install_logits)\n",
    "        return click_logits, install_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelV1_DEP(nn.Module):\n",
    "    CATEG_COLS = revised_categ_cols\n",
    "    BINARY_COLS = revised_binary_cols\n",
    "    NUM_COLS = revised_num_cols\n",
    "    \n",
    "    START_COL_IDX = 2\n",
    "    \n",
    "    BINARY_COLS_IDX = range(START_COL_IDX + len(CATEG_COLS), START_COL_IDX + len(CATEG_COLS) + len(BINARY_COLS))\n",
    "    \n",
    "    NUM_COLS_IDX = range(BINARY_COLS_IDX[-1] + 1, BINARY_COLS_IDX[-1] + 1 + len(NUM_COLS))\n",
    "    \n",
    "    CATEG_EMB_DIM = 32\n",
    "    BINARY_FC_OUT = 32\n",
    "    NUM_FC_OUT = 32\n",
    "\n",
    "    COMB_FC_OUT_1 = 128\n",
    "    NUM_CLASSES = 1\n",
    "\n",
    "    def __init__(self, df):\n",
    "        super(ModelV1_DEP, self).__init__()\n",
    "        # categ feats\n",
    "        cat_emb = [None]*len(self.CATEG_COLS)\n",
    "        self.emb_sizes = []\n",
    "        for col, col_idx in zip(self.CATEG_COLS, range(len(self.CATEG_COLS))):\n",
    "            n_emb = max(categ_feat_to_id[col].values())+1\n",
    "            emb_size = int(6 * (n_emb ** 0.25))\n",
    "            cat_emb[col_idx] = nn.Embedding(n_emb, emb_size).to(DEVICE)\n",
    "            self.emb_sizes.append(emb_size)\n",
    "            \n",
    "        self.cat_emb = nn.ModuleList(cat_emb)\n",
    "        \n",
    "        bin_emb = [None]*len(self.BINARY_COLS)\n",
    "        self.bin_emb_sizes = []\n",
    "        for col, col_idx in zip(self.BINARY_COLS, range(len(self.BINARY_COLS))):\n",
    "            n_emb = max(categ_feat_to_id[col].values())+1\n",
    "            self.bin_emb_sizes.append(1)\n",
    "            bin_emb[col_idx] = nn.Embedding(n_emb, 1).to(DEVICE)\n",
    "        self.bin_emb = nn.ModuleList(bin_emb)\n",
    "\n",
    "        # numerical feats\n",
    "        self.num_fc = nn.Sequential(\n",
    "            nn.Linear(len(self.NUM_COLS), self.NUM_FC_OUT),\n",
    "            nn.BatchNorm1d(self.BINARY_FC_OUT),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # final cls layers\n",
    "        self.fc_intm_click = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(\n",
    "                sum(self.emb_sizes) + sum(self.bin_emb_sizes) + self.NUM_FC_OUT,\n",
    "                self.COMB_FC_OUT_1\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc_out_click = nn.Linear(self.COMB_FC_OUT_1, self.NUM_CLASSES)\n",
    "        \n",
    "        self.fc_out_install = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(\n",
    "                sum(self.emb_sizes) + sum(self.bin_emb_sizes) + self.NUM_FC_OUT + self.COMB_FC_OUT_1,\n",
    "                self.COMB_FC_OUT_1\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.COMB_FC_OUT_1, self.NUM_CLASSES)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, return_click=True):\n",
    "        cat_emb_out = [None]*len(self.CATEG_COLS)\n",
    "        for col_idx in range(len(self.CATEG_COLS)):\n",
    "            col_x = x[:, self.START_COL_IDX + col_idx].long()\n",
    "            cat_emb_out[col_idx] = self.cat_emb[col_idx](col_x)\n",
    "        cat_emb_out = torch.concat(cat_emb_out, axis=1)\n",
    "\n",
    "        bin_out = [None]*len(self.BINARY_COLS)\n",
    "        for col_idx in range(len(self.BINARY_COLS)):\n",
    "            col_x = x[:, self.START_COL_IDX + len(self.CATEG_COLS) + col_idx].long()\n",
    "            bin_out[col_idx] = self.bin_emb[col_idx](col_x)\n",
    "        bin_out = torch.concat(bin_out, axis=1)\n",
    "        \n",
    "        num_out = self.num_fc(x[:, self.NUM_COLS_IDX])\n",
    "\n",
    "        comb = torch.concat([cat_emb_out, bin_out, num_out], axis=1)\n",
    "        \n",
    "        click_fc_intm = self.fc_intm_click(comb)\n",
    "        \n",
    "        install_fc_inp = torch.cat([comb, click_fc_intm], axis=1)\n",
    "        install_fc_out = self.fc_out_install(install_fc_inp)\n",
    "        \n",
    "        install_logits = torch.sigmoid(install_fc_out)\n",
    "        \n",
    "        if return_click:\n",
    "            click_fc_out = self.fc_out_click(click_fc_intm)\n",
    "            click_logits = torch.sigmoid(click_fc_out)\n",
    "        else:\n",
    "            click_logits = torch.zeros_like(install_logits)\n",
    "        \n",
    "        return click_logits, install_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "\n",
    "# model_v1 = ModelV1(_final_df).to(DEVICE)\n",
    "# model_v1_dep = ModelV1_DEP(_final_df).to(DEVICE)\n",
    "model_v1_bin = ModelV1_BIN(_final_df).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for model \n",
    "\n",
    "def save_ckpt(model, optim, hist, ckpt_no, exp_name='/rs23'):\n",
    "    \"Function to save ckpt of ckpt_no and experiment name\"\n",
    "    data = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optim_state_dict': optim.state_dict(),\n",
    "        'hist': hist\n",
    "    }\n",
    "    ckpt_base = './models/'\n",
    "    os.makedirs(os.path.join(ckpt_base, exp_name), exist_ok=True)\n",
    "    \n",
    "    save_path = os.path.join(ckpt_base, exp_name, f'model_{ckpt_no}.pth.tar')\n",
    "    torch.save(data, save_path)\n",
    "    print('Saved model ckpt - {}'.format(save_path), end='\\n\\n')\n",
    "\n",
    "def resume_ckpt(model, optim, ckpt_no, exp_name='/rs23'):\n",
    "    \"Function to load ckpt given ckpt_no and experiment name\"\n",
    "    ckpt_base = './models/'\n",
    "    load_path = os.path.join(ckpt_base, exp_name, f'model_{ckpt_no}.pth.tar')\n",
    "    print('loading model ckpt - {}'.format(load_path), end='\\n\\n')\n",
    "\n",
    "    data = torch.load(load_path)\n",
    "    \n",
    "    model.load_state_dict(data['model_state_dict'])\n",
    "    \n",
    "    if data.get('optim_state_dict', None) is not None:\n",
    "        optim.load_state_dict(data['optim_state_dict'])\n",
    "    if data.get('hist', None) is not None:\n",
    "        hist = data['hist']\n",
    "    else:\n",
    "        hist = None\n",
    "\n",
    "    return ckpt_no, model, optim, hist\n",
    "\n",
    "def log_loss_metric(model_out, target, idx=1):\n",
    "    \"\"\"Normalized Log Loss as provided by organizers\"\"\"\n",
    "    return metrics.log_loss(target[:, idx].cpu().detach().numpy().reshape(-1), \n",
    "                           model_out[idx].cpu().detach().numpy().reshape(-1),\n",
    "                           labels=[0,1], eps=1e-7, normalize=True)\n",
    "\n",
    "\n",
    "def loss_fn(model_out, target, criterion, idx=None):\n",
    "    \"\"\"\n",
    "    Loss function \n",
    "        L = L_{is_clicked} + L_{is_installed}\n",
    "        \n",
    "    Args:\n",
    "        model_out: output from model\n",
    "        target: target tensor\n",
    "        criterion: criterion for indiv output\n",
    "        idx: (optional) if idx is not None then default Loss function \n",
    "                else loss func comprise criterion value of itemss at `idx` in model output\n",
    "    \"\"\"\n",
    "    if idx is None:\n",
    "        loss = 0.\n",
    "        for i, logits in enumerate(model_out):\n",
    "            loss += criterion(logits, target[:, i].reshape(-1, 1))\n",
    "    else:\n",
    "        loss = criterion(model_out[idx], target[:, idx].reshape(-1, 1))\n",
    "    return loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "1. Use of BinaryCrossEntropy criterion for `is_installed` and `is_clicked` predictions\n",
    "2. We utilize two training approaches\n",
    "    \n",
    "    a. only train on `is_installed` \n",
    "        $$ \\mathcal{L}=\\mathcal{L}_{is\\_installed} $$\n",
    "        \n",
    "    b. Optimize jointly on `is_installed` and `is_clicked`.\n",
    "        $$ \\mathcal{L}=\\mathcal{L}_{is\\_installed} + \\mathcal{L}_{is\\_clicked} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment Config class\n",
    "class Config:\n",
    "    exp_name = 'None'\n",
    "    \n",
    "    resume = False\n",
    "    ckpt_no = 0\n",
    "    epoch = 0\n",
    "    n_epochs = 40\n",
    "    \n",
    "    only_install = True\n",
    "    lr = 0.005\n",
    "    loss_fn = partial(loss_fn, criterion=nn.BCELoss())\n",
    "    \n",
    "    only_is_install = True\n",
    "    use_valid = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    \"\"\"\n",
    "    Trainer class to run the experiments using passed config\n",
    "    \n",
    "    Args:\n",
    "        config: exp config (Config object)\n",
    "        model: pytorch model\n",
    "        trainlaoder: training dataloader\n",
    "        validloader: (optional) validation dataloader\n",
    "    \"\"\"\n",
    "    def __init__(self, config, model, trainloader, validloader):\n",
    "        self.config = config\n",
    "        self.trainloader = trainloader\n",
    "        if config.use_valid:\n",
    "            self.validloader = validloader\n",
    "        else:\n",
    "            self.validloader = None\n",
    "        \n",
    "        self.exp_name = config.exp_name\n",
    "        self.epoch = config.epoch\n",
    "        self.n_epoch = config.n_epochs\n",
    "        self.ckpt_no = config.ckpt_no\n",
    "        \n",
    "        self.hist = self.init_hist()\n",
    "        self.model = model\n",
    "        self.optim = optim.Adam(self.model.parameters(), lr=config.lr)\n",
    "        \n",
    "        self.loss_fn = config.loss_fn\n",
    "        \n",
    "        if config.resume:\n",
    "            self.load(config.ckpt_no)\n",
    "            \n",
    "    def load(self, ckpt_no):\n",
    "        \"Load checkpoint from ckpt_no\"\n",
    "        self.epoch, self.model, self.optim, self.hist = resume_ckpt(\n",
    "            self.model, self.optim, ckpt_no, self.exp_name)\n",
    "        if self.hist is None:\n",
    "            self.init_hist()\n",
    "         \n",
    "        \n",
    "    def init_hist(self):\n",
    "        \"initialize history to track metrics\"\n",
    "        modes = ['train', 'valid', 'test']\n",
    "        metrics = ['iter', 'loss', 'loss-std', 'log_loss', 'log_loss-std']\n",
    "        hist = {}\n",
    "        for mode in modes:\n",
    "            hist[mode] = {}\n",
    "            for metric in metrics:\n",
    "                hist[mode][metric] = []\n",
    "        return hist\n",
    "    \n",
    "    def log(self, mode, loss, log_loss):\n",
    "        \"log metrics in history\"\n",
    "        self.hist[mode]['iter'].append(self.epoch)\n",
    "        self.hist[mode]['loss'].append(np.mean(loss))\n",
    "        self.hist[mode]['loss-std'].append(np.std(loss))\n",
    "        self.hist[mode]['log_loss'].append(np.mean(log_loss))\n",
    "        self.hist[mode]['log_loss-std'].append(np.std(log_loss))\n",
    "        \n",
    "    def save(self):\n",
    "        \"Save model checkpoint\"\n",
    "        save_ckpt(self.model, self.optim, self.hist, self.epoch, self.exp_name)\n",
    "    \n",
    "    def train(self, ):\n",
    "        \"Training Loop\"\n",
    "        for self.epoch in range(1 + self.ckpt_no, self.n_epoch + 1 + self.ckpt_no):\n",
    "            self.model.train()\n",
    "            losses = []\n",
    "            log_losses = []\n",
    "\n",
    "            pbar = tqdm(self.trainloader, leave=False, total=len(self.trainloader))\n",
    "            for batch in pbar:\n",
    "                self.optim.zero_grad()\n",
    "\n",
    "                batch_x, batch_y = batch\n",
    "                batch_x = batch_x.to(DEVICE)\n",
    "                batch_y = batch_y.to(DEVICE)\n",
    "\n",
    "                out = self.model(batch_x, not self.config.only_is_install)\n",
    "\n",
    "                loss = self.loss_fn(out, batch_y, \n",
    "                                    idx=None if not self.config.only_is_install else 1)\n",
    "                losses.append(loss.item())\n",
    "                log_losses.append(log_loss_metric(out, batch_y))\n",
    "\n",
    "                loss.backward()\n",
    "                self.optim.step()\n",
    "\n",
    "            self.log('train', losses, log_losses)\n",
    "            \n",
    "            if self.config.use_valid:\n",
    "                valid_losses, valid_log_losses = self.valid_loop()\n",
    "                print(\"epoch: {} | loss: {:0.5f} | log_loss: {:0.5f} | \\\n",
    "                        valid_loss: {:0.5f} | valid_log_loss: {:0.5f}\" \\\n",
    "                      .format(self.epoch, np.mean(losses), np.mean(log_losses), \n",
    "                              np.mean(valid_losses), np.mean(valid_log_losses)))\n",
    "\n",
    "                if np.mean(valid_losses) <= min(self.hist['valid']['loss']):\n",
    "                    self.save()\n",
    "            else:\n",
    "                print(\"epoch: {} | loss: {:0.5f} | log_loss: {:0.5f}\" \\\n",
    "                      .format(self.epoch, np.mean(losses), np.mean(log_losses)))\n",
    "                \n",
    "                if np.mean(losses) <= min(self.hist['train']['loss']):\n",
    "                    self.save()\n",
    "        self.save()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def valid_loop(self,):\n",
    "        \"Validation loop\"\n",
    "        self.model.eval()\n",
    "        losses = []\n",
    "        log_losses = []\n",
    "\n",
    "        pbar = tqdm(self.validloader, leave=False, total=len(self.validloader))\n",
    "        for batch in pbar:\n",
    "            batch_x, batch_y = batch\n",
    "            batch_x = batch_x.to(DEVICE)\n",
    "            batch_y = batch_y.to(DEVICE)\n",
    "\n",
    "            out = self.model(batch_x, not self.config.only_is_install)\n",
    "\n",
    "            loss = self.loss_fn(out, batch_y, \n",
    "                                idx=None if not self.config.only_is_install else 1)\n",
    "            losses.append(loss.item())\n",
    "            log_losses.append(log_loss_metric(out, batch_y))\n",
    "\n",
    "        self.log('valid', losses, log_losses)\n",
    "        return losses, log_losses\n",
    "    \n",
    "    def get_best_model(self):\n",
    "        best_ckpt_no = self.hist['valid']['iter'][\n",
    "                            self.hist['valid']['loss'].index(min(self.hist['valid']['loss']))\n",
    "                        ]\n",
    "        self.load(best_ckpt_no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(hist, metrics = ['loss', 'log_loss']):\n",
    "    \"Helper function to plot training statistics\"\n",
    "    fig, _ax = plt.subplots(1, len(metrics), figsize=(5*len(metrics), 3))\n",
    "    modes = ['train', 'valid']\n",
    "    for i, metric in enumerate(metrics):\n",
    "        ax = _ax[i] if len(metrics) > 1 else _ax\n",
    "        colors = ['grey', 'blue']\n",
    "        colors = iter(colors)\n",
    "        for mode in modes:\n",
    "            color = next(colors)\n",
    "            x_axis = hist[mode]['iter']\n",
    "            y_axis = np.array(hist[mode][metric])\n",
    "            \n",
    "            if metric+'-std' in hist[mode]:\n",
    "                y_axis_std = np.array(hist[mode][metric+'-std'])\n",
    "                ax.fill_between(x_axis, y_axis-y_axis_std, y_axis+y_axis_std, \n",
    "                                   alpha=0.2, color=color)\n",
    "            ax.plot(x_axis, y_axis, alpha=0.9, color=color, label=mode)\n",
    "            \n",
    "        ax.set_xlabel('epoch')\n",
    "        ax.set_ylabel(metric)\n",
    "        ax.grid()\n",
    "        ax.legend()\n",
    "    \n",
    "    if len(metrics)>1: plt.subplots_adjust(wspace=0.3)\n",
    "    plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best res cell copy\n",
    "config_binEmb_allData = Config()\n",
    "config_binEmb_allData.exp_name = \"sub_model_v1_emb_binEmb_removal_low_corr_custom_col_nanMeanImpute_only_install_num_to_cat_dynEmb_bin32_num32_alldata_copy\"\n",
    "config_binEmb_allData.n_epochs = 41\n",
    "config_binEmb_allData.lr = 5e-4\n",
    "config_binEmb_allData.use_valid = False\n",
    "\n",
    "# config_binEmb_allData.resume=True\n",
    "# config_binEmb_allData.ckpt_no = 46\n",
    "\n",
    "trainer_binEmb_allData = Trainer(config_binEmb_allData, model_v1_bin, trainloader, None)\n",
    "trainer_binEmb_allData.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gen Submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best model\n",
    "# trainer_binEmb_allData.get_best_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_preds(testloader, model):\n",
    "    \"get predictions from model on test data\"\n",
    "    testit = next(iter(testloader)).to(DEVICE)\n",
    "    model.eval()\n",
    "    testout = model(testit, False)\n",
    "    return testout, testit\n",
    "\n",
    "testout, testit = get_preds(testloader, trainer_binEmb_allData.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format submission columns\n",
    "_test_df['f_0'] = testit[:,0].cpu().detach().numpy().astype(int)\n",
    "_test_df['is_clicked'] = testout[0].cpu().detach().numpy()\n",
    "_test_df['is_installed'] = testout[1].cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sub(df):\n",
    "    \"Generate submission df\"\n",
    "    cols = ['f_0', 'is_clicked', 'is_installed']\n",
    "    res_df = df[cols]\n",
    "    res_df = res_df.rename(columns={'f_0':'RowId'})\n",
    "    return res_df\n",
    "\n",
    "# save submission csv\n",
    "sub_path = os.path.join('./sub', _trainer_pow_alldata.exp_name + '10epochs' + '.csv')\n",
    "os.makedirs('./sub', exist_ok=True)\n",
    "get_sub(_test_df).to_csv(sub_path, sep='\\t', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
